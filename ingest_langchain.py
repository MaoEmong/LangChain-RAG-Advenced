"""
ingest_langchain.py
============================================================
ë¬¸ì„œ ìˆ˜ì§‘ ë° ë²¡í„°DB êµ¬ì¶• ìŠ¤í¬ë¦½íŠ¸

ì´ íŒŒì¼ì˜ ì—­í•  (RAGì—ì„œ ë§¤ìš° ì¤‘ìš”):
------------------------------------------------------------
âœ” docs í´ë” ì•ˆì˜ ë‹¤ì–‘í•œ ë¬¸ì„œ íŒŒì¼ì„ ì½ëŠ”ë‹¤
âœ” ëª¨ë“  íŒŒì¼ì„ LangChainì˜ Document í˜•íƒœë¡œ í†µì¼í•œë‹¤
âœ” ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ chunkë¡œ ìª¼ê° ë‹¤
âœ” ê° chunkë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•´ì„œ Chroma ë²¡í„°DBì— ì €ì¥í•œë‹¤

ì¦‰,
ğŸ‘‰ "RAGì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆëŠ” ë°ì´í„°"ë¥¼ ë¯¸ë¦¬ ë§Œë“¤ì–´ë‘ëŠ” ë‹¨ê³„

ì‹¤í–‰:
  python ingest_langchain.py

ì£¼ì˜:
- ì„œë²„ ì½”ë“œê°€ ì•„ë‹˜ (ë…ë¦½ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸)
- API ì½”ë“œê°€ ì•„ë‹˜
- ë¬¸ì„œê°€ ë°”ë€Œì—ˆì„ ë•Œë§Œ ì‹¤í–‰í•˜ë©´ ë¨
- ì‹¤í–‰ í›„ ë²¡í„°DBê°€ ìƒì„±/ì—…ë°ì´íŠ¸ë¨
"""


# ----------------------------
# íŒŒì´ì¬ ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
# ----------------------------
import os      # ê²½ë¡œ ì²˜ë¦¬, í´ë” ìƒì„±
import glob    # í´ë” ì•ˆ íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰
import uuid


# ----------------------------
# LangChain í•µì‹¬ ìë£Œêµ¬ì¡°
# ----------------------------

# Document:
# - LangChainì—ì„œ ì‚¬ìš©í•˜ëŠ” "ë¬¸ì„œ í‘œì¤€ í˜•íƒœ"
# - page_content : ì‹¤ì œ í…ìŠ¤íŠ¸
# - metadata     : ì¶œì²˜, í˜ì´ì§€ ë²ˆí˜¸, ê¸°íƒ€ ì •ë³´
from langchain_core.documents import Document


# RecursiveCharacterTextSplitter:
# - ê¸´ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì€ ë‹¨ìœ„(chunk)ë¡œ ìª¼ê°œëŠ” ë„êµ¬
from langchain_text_splitters import RecursiveCharacterTextSplitter


# OpenAIEmbeddings:
# - í…ìŠ¤íŠ¸ â†’ ìˆ«ì ë²¡í„°(ì„ë² ë”©)ë¡œ ë³€í™˜
from langchain_openai import OpenAIEmbeddings


# Chroma:
# - ë¡œì»¬ íŒŒì¼ ê¸°ë°˜ ë²¡í„°DB
from langchain_chroma import Chroma


# ----------------------------
# ë¬¸ì„œ ë¡œë”ë“¤ (íŒŒì¼ íƒ€ì…ë³„)
# ----------------------------
# ê° íŒŒì¼ì„ ì½ì–´ì„œ Document ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¤ì–´ì£¼ëŠ” ì—­í• 
from langchain_community.document_loaders import (
    TextLoader,                 # .txt
    UnstructuredMarkdownLoader, # .md
    PyPDFLoader,                # .pdf
    Docx2txtLoader,             # .docx
    BSHTMLLoader,               # .html / .htm
)

from langchain_chroma import Chroma
from langchain_community.document_loaders import TextLoader

from langchain_classic.retrievers import ParentDocumentRetriever
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from docstore_sqlite import SQLiteDocStore
from config import DOCSTORE_PATH

# ----------------------------
# í”„ë¡œì íŠ¸ ê³µí†µ ì„¤ì •
# ----------------------------
# config.pyì— ì •ì˜ëœ ê°’ë“¤
from config import EMBED_MODEL, OPENAI_API_KEY, CHUNK_SIZE, CHUNK_OVERLAP


# ============================================================
# ì´ íŒŒì¼ ì „ìš© ì„¤ì •ê°’
# ============================================================

# ë¬¸ì„œê°€ ë“¤ì–´ìˆëŠ” í´ë”
DOCS_DIR = "./docs"

# ë²¡í„°DBê°€ ì €ì¥ë  í´ë”
CHROMA_DIR = "./chroma_db"

# Chroma ë‚´ë¶€ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì»¬ë ‰ì…˜ ì´ë¦„
COLLECTION_NAME = "my_rag_docs"

# ============================================================
# 1ï¸âƒ£ ë¬¸ì„œ ë¡œë”© ë‹¨ê³„ (ë¡œë” í™•ì¥ ë²„ì „)
# ============================================================
def load_docs_from_folder(folder: str) -> list[Document]:
    """
    docs í´ë” ì•ˆì˜ íŒŒì¼ë“¤ì„ í™•ì¥ìë³„ ë¡œë”ë¡œ ì½ì–´ì„œ
    LangChain Document ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ”:
    1. ì§€ì •ëœ í´ë”ë¥¼ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
    2. íŒŒì¼ í™•ì¥ìì— ë§ëŠ” ë¡œë” ì„ íƒ
    3. ê° íŒŒì¼ì„ Document ê°ì²´ë¡œ ë³€í™˜
    4. ë©”íƒ€ë°ì´í„°ì— ì¶œì²˜ ì •ë³´ ì¶”ê°€

    ì§€ì› í™•ì¥ì:
    - .txt: í…ìŠ¤íŠ¸ íŒŒì¼
    - .md: ë§ˆí¬ë‹¤ìš´ íŒŒì¼
    - .pdf: PDF ë¬¸ì„œ
    - .docx: Word ë¬¸ì„œ
    - .html / .htm: HTML íŒŒì¼

    Args:
        folder (str): ë¬¸ì„œê°€ ìˆëŠ” í´ë” ê²½ë¡œ

    Returns:
        list[Document]: ë³€í™˜ëœ Document ê°ì²´ ë¦¬ìŠ¤íŠ¸

    Note:
        - íŒŒì¼ ë¡œë”© ì‹¤íŒ¨ ì‹œ ê²½ê³ ë§Œ ì¶œë ¥í•˜ê³  ê³„ì† ì§„í–‰
        - ìƒˆë¡œìš´ íŒŒì¼ íƒ€ì… ì¶”ê°€ ì‹œ loader_rulesì— ì¶”ê°€í•˜ë©´ ë¨
    """
    docs: list[Document] = []

    # (í™•ì¥ì, ë¡œë” ìƒì„± í•¨ìˆ˜) ë§¤í•‘
    # ìƒˆë¡œìš´ íŒŒì¼ íƒ€ì…ì„ ì¶”ê°€í•˜ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° í•œ ì¤„ë§Œ ì¶”ê°€í•˜ë©´ ë¨
    loader_rules = [
        (".txt",  lambda p: TextLoader(p, encoding="utf-8")),
        (".md",   lambda p: TextLoader(p, encoding="utf-8")),
        (".pdf",  lambda p: PyPDFLoader(p)),
        (".docx", lambda p: Docx2txtLoader(p)),
        (".html", lambda p: BSHTMLLoader(p)),
        (".htm",  lambda p: BSHTMLLoader(p)),
    ]

    # docs í´ë” ì•„ë˜ ëª¨ë“  íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ íƒìƒ‰
    # **/* íŒ¨í„´ìœ¼ë¡œ í•˜ìœ„ í´ë”ê¹Œì§€ ëª¨ë‘ íƒìƒ‰
    for path in glob.glob(os.path.join(folder, "**/*"), recursive=True):

        # íŒŒì¼ì´ ì•„ë‹ˆë©´(í´ë”ë©´) ë¬´ì‹œ
        if not os.path.isfile(path):
            continue

        # í™•ì¥ì ì¶”ì¶œ (.pdf, .txt ë“±) - ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ëŒ€ì†Œë¬¸ì êµ¬ë¶„ ì—†ì´ ë§¤ì¹­
        ext = os.path.splitext(path)[1].lower()

        # í™•ì¥ìì— ë§ëŠ” ë¡œë” ì°¾ê¸°
        for rule_ext, make_loader in loader_rules:
            if ext == rule_ext:
                try:
                    # ë¡œë” ìƒì„± (íŒŒì¼ íƒ€ì…ë³„ë¡œ ì ì ˆí•œ ë¡œë” ì‚¬ìš©)
                    loader = make_loader(path)

                    # íŒŒì¼ì„ ì½ì–´ì„œ Document ë¦¬ìŠ¤íŠ¸ ìƒì„±
                    # PDFëŠ” í˜ì´ì§€ë³„ë¡œ, í…ìŠ¤íŠ¸ëŠ” ì „ì²´ë¡œ Document ìƒì„±
                    loaded_docs = loader.load()

                    # source ë©”íƒ€ë°ì´í„°ë¥¼ "íŒŒì¼ ê²½ë¡œ"ë¡œ í†µì¼
                    # ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ ì¼ê´€ì„± ìœ ì§€
                    abs_path = os.path.abspath(path)
                    for d in loaded_docs:
                        d.metadata["source"] = abs_path

                    # ê²°ê³¼ ëˆ„ì 
                    docs.extend(loaded_docs)

                except Exception as e:
                    # íŒŒì¼ í•˜ë‚˜ê°€ ê¹¨ì ¸ ìˆì–´ë„ ì „ì²´ ingestê°€ ë©ˆì¶”ì§€ ì•Šê²Œ í•¨
                    # (ì˜ˆ: ì•”í˜¸í™”ëœ PDF, ì†ìƒëœ íŒŒì¼ ë“±)
                    print(f"[WARN] failed to load: {path} ({e})")

                # ë¡œë” ì°¾ì•˜ìœ¼ë©´ ë‹¤ìŒ íŒŒì¼ë¡œ ì´ë™
                break

    return docs


# ============================================================
# 2ï¸âƒ£ ì²­í‚¹ ë‹¨ê³„
# ============================================================
def chunk_docs(docs: list[Document]) -> list[Document]:
    """
    ê¸´ Documentë“¤ì„ ì‘ì€ chunk Documentë“¤ë¡œ ìª¼ê°­ë‹ˆë‹¤.

    RAG ì‹œìŠ¤í…œì—ì„œ ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì´ìœ :
    1. ê²€ìƒ‰ ì •í™•ë„ í–¥ìƒ: ê´€ë ¨ ë¶€ë¶„ë§Œ ì •í™•íˆ ì°¾ì„ ìˆ˜ ìˆìŒ
    2. ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ: LLMì˜ í† í° ì œí•œ ë‚´ì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥
    3. íš¨ìœ¨ì„±: í•„ìš”í•œ ë¶€ë¶„ë§Œ ê²€ìƒ‰í•˜ì—¬ ë¹„ìš© ì ˆê°

    Args:
        docs (list[Document]): ì›ë³¸ Document ë¦¬ìŠ¤íŠ¸

    Returns:
        list[Document]: ì²­í‚¹ëœ Document ë¦¬ìŠ¤íŠ¸

    Note:
        - RecursiveCharacterTextSplitterëŠ” ë¬¸ë‹¨, ë¬¸ì¥, ë‹¨ì–´ ë‹¨ìœ„ë¡œ
          ìì—°ìŠ¤ëŸ½ê²Œ ë¶„í• í•˜ì—¬ ë¬¸ë§¥ì„ ìµœëŒ€í•œ ë³´ì¡´í•©ë‹ˆë‹¤
        - chunk_overlapì„ ì‚¬ìš©í•˜ì—¬ ì•ë’¤ ë¬¸ë§¥ì´ ëŠê¸°ì§€ ì•Šë„ë¡ í•©ë‹ˆë‹¤
    """
    # RecursiveCharacterTextSplitter ì´ˆê¸°í™”
    # - chunk_size: ê° ì²­í¬ì˜ ìµœëŒ€ ë¬¸ì ìˆ˜
    # - chunk_overlap: ì²­í¬ ê°„ ê²¹ì¹˜ëŠ” ë¬¸ì ìˆ˜ (ë¬¸ë§¥ ë³´ì¡´)
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )

    # ì…ë ¥  : [Document, Document, ...] (ì›ë³¸ ë¬¸ì„œë“¤)
    # ì¶œë ¥  : [chunked Document, chunked Document, ...] (ì‘ì€ ì²­í¬ë“¤)
    return splitter.split_documents(docs)

def build_parent_retriever(db: Chroma) -> ParentDocumentRetriever:
    """Parent/Child êµ¬ì¡°ë¥¼ êµ¬ì„±í•´ì£¼ëŠ” retriever íŒ©í† ë¦¬."""
    docstore = SQLiteDocStore(DOCSTORE_PATH)

    parent_splitter = RecursiveCharacterTextSplitter(
        chunk_size=2000,
        chunk_overlap=200,
    )
    child_splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP,
    )

    return ParentDocumentRetriever(
        vectorstore=db,
        docstore=docstore,
        child_splitter=child_splitter,
        parent_splitter=parent_splitter,
        parent_id_key="doc_id",
    )

# ============================================================
# 3ï¸âƒ£ ë²¡í„°DB ì €ì¥ ë‹¨ê³„
# ============================================================
def build_or_update_chroma(chunks: list[Document]) -> None:
    """
    chunk Documentë“¤ì„ ì„ë² ë”©í•´ì„œ Chroma ë²¡í„°DBì— ì €ì¥í•©ë‹ˆë‹¤.

    ì´ í•¨ìˆ˜ëŠ”:
    1. OpenAI ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
    2. ChromaDB ë²¡í„° ì €ì¥ì†Œ ìƒì„±/ë¡œë“œ
    3. ê° ì²­í¬ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥

    Args:
        chunks (list[Document]): ì €ì¥í•  ì²­í¬ Document ë¦¬ìŠ¤íŠ¸

    Returns:
        None

    Note:
        - ì´ë¯¸ ë²¡í„°DBê°€ ìˆìœ¼ë©´ ê¸°ì¡´ ë°ì´í„°ì— ì¶”ê°€ë©ë‹ˆë‹¤
        - persist_directoryì— ìë™ìœ¼ë¡œ íŒŒì¼ì´ ì €ì¥ë©ë‹ˆë‹¤
        - ì„ë² ë”© ë³€í™˜ì€ OpenAI APIë¥¼ í˜¸ì¶œí•˜ë¯€ë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤
    """
    # OpenAI ì„ë² ë”© ê°ì²´ ìƒì„±
    # í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜í•˜ëŠ” ë° ì‚¬ìš©
    embeddings = OpenAIEmbeddings(
        model=EMBED_MODEL,
        api_key=OPENAI_API_KEY
    )

    # Chroma ë²¡í„°DB ë¡œë“œ ë˜ëŠ” ìƒì„±
    # - collection_name: ì €ì¥ì†Œ ë‚´ë¶€ì˜ ì»¬ë ‰ì…˜ ì´ë¦„
    # - embedding_function: ë²¡í„° ë³€í™˜ í•¨ìˆ˜
    # - persist_directory: ë²¡í„°DB íŒŒì¼ ì €ì¥ ê²½ë¡œ (ìë™ ì €ì¥ë¨)
    db = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_DIR,
    )

    # chunk Documentë“¤ì„ ë²¡í„°DBì— ì¶”ê°€
    # ê° ì²­í¬ì˜ í…ìŠ¤íŠ¸ê°€ ì„ë² ë”©ë˜ì–´ ë²¡í„°ë¡œ ë³€í™˜ë˜ê³  ì €ì¥ë¨
    db.add_documents(chunks)

def build_or_load_chroma() -> Chroma:
    embeddings = OpenAIEmbeddings(model=EMBED_MODEL, api_key=OPENAI_API_KEY)
    db = Chroma(
        collection_name=COLLECTION_NAME,
        embedding_function=embeddings,
        persist_directory=CHROMA_DIR,
    )
    return db


# ============================================================
# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ============================================================
def main():
    """
    ingest_langchain.py ì‹¤í–‰ ì‹œ ì—¬ê¸°ë¶€í„° ì‹œì‘ë©ë‹ˆë‹¤.

    ë©”ì¸ ì‹¤í–‰ íë¦„:
    1. í•„ìš”í•œ í´ë” ìƒì„±
    2. ë¬¸ì„œ ë¡œë”© (ë‹¤ì–‘í•œ íŒŒì¼ í˜•ì‹ ì§€ì›)
    3. ë¬¸ì„œ ì²­í‚¹ (ê¸´ ë¬¸ì„œë¥¼ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• )
    4. ë²¡í„°DB ì €ì¥ (ì„ë² ë”© ë³€í™˜ ë° ì €ì¥)

    Returns:
        None
    """
    # docs / chroma_db í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    # exist_ok=True: ì´ë¯¸ ì¡´ì¬í•´ë„ ì—ëŸ¬ ë°œìƒ ì•ˆ í•¨
    os.makedirs(DOCS_DIR, exist_ok=True)
    os.makedirs(CHROMA_DIR, exist_ok=True)

    # 1ë‹¨ê³„: ë¬¸ì„œ ë¡œë”©
    # docs í´ë”ì˜ ëª¨ë“  ë¬¸ì„œë¥¼ LangChain Documentë¡œ ë³€í™˜
    docs = load_docs_from_folder(DOCS_DIR)
    if not docs:
        print(f"[WARN] no docs found in {DOCS_DIR}")
        return

    # âœ… parent_id(doc_id) ë¶€ì—¬(í•„ìˆ˜)
    for d in docs:
        d.metadata["doc_id"] = str(uuid.uuid4())

    db = build_or_load_chroma()
    retriever = build_parent_retriever(db)

    # âœ… í•µì‹¬: parentëŠ” SQLiteì—, childëŠ” Chromaì— ë“¤ì–´ê°
    retriever.add_documents(docs)

    print(f"[OK] loaded docs: {len(docs)} (parents stored in sqlite, children in chroma)")


# ============================================================
# íŒŒì´ì¬ íŒŒì¼ ì§ì ‘ ì‹¤í–‰ ì‹œ ì‹œì‘ ì§€ì 
# ============================================================
if __name__ == "__main__":
    main()
